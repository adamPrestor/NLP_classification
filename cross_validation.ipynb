{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import functools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from preprocessing import read_dataset\n",
    "from preprocessing import Tokenization, StopWordsRemover, Lemmatization, RoofRemoval, SpellingCorrection\n",
    "from preprocessing import GibberishDetector, TokenGrouping, TokenDictionary, SentimentAnalysis\n",
    "\n",
    "from csv_parser import split_train_test\n",
    "\n",
    "import features as F\n",
    "from models import MajorityModel, SklearnModel, CRFModel\n",
    "from cross_validation import cross_validate\n",
    "from evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "dataset_path = 'data/discussion_data.csv'\n",
    "df = read_dataset(dataset_path)\n",
    "df['Message Time'] = pd.to_datetime(df['Message Time'])\n",
    "tags = df.CategoryBroad.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, lemmatization, roof removal\n",
    "tokenizer = Tokenization()\n",
    "stop_words_remover = StopWordsRemover('data/stopwords-sl-custom.txt')\n",
    "lemmatizer = Lemmatization()\n",
    "roof_removal = RoofRemoval()\n",
    "\n",
    "# spelling_correction = SpellingCorrection('data/dict-sl.txt', roof_removal)\n",
    "\n",
    "# Gibberish detector\n",
    "gibberish_detector = GibberishDetector(roof_removal)\n",
    "gibberish_detector.train('data/dict-sl.txt', 'data/gibberish_good.txt', 'data/gibberish_bad.txt')\n",
    "\n",
    "# Token grouping\n",
    "token_grouping = TokenGrouping(gibberish_detector)\n",
    "\n",
    "sa = SentimentAnalysis('data/negative_words_Slolex.txt', 'data/positive_words_Slolex.txt', roof_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the books\n",
    "book_tokens = {}\n",
    "for book in os.listdir('data/books'):\n",
    "    book_id = int(book.split('.')[0])\n",
    "    with io.open(os.path.join('data/books', book), mode='r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        tokens = tokenizer.tokenize(content)\n",
    "        tokens = stop_words_remover.remove_stopwords(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [roof_removal.remove(token) for token in tokens]\n",
    "        tokens = [token_grouping.group_tokens(token) for token in tokens]\n",
    "        # remove gibberish and punctuations\n",
    "        tokens = [token for token in tokens if token.isalpha() and not token == '<other>']\n",
    "        book_tokens[book_id] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the topics\n",
    "topics_tokens = {}\n",
    "for topic in df['Topic'].unique():\n",
    "    tokens = tokenizer.tokenize(topic)\n",
    "    tokens = stop_words_remover.remove_stopwords(tokens)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [roof_removal.remove(token) for token in tokens]\n",
    "    tokens = [token_grouping.group_tokens(token) for token in tokens]\n",
    "    # remove gibberish and punctuations\n",
    "    tokens = [token for token in tokens if token.isalpha() and not token == '<other>']\n",
    "    topics_tokens[topic] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the messages\n",
    "messages_sent = []\n",
    "for message in df.Message:\n",
    "    tokens = tokenizer.tokenize(message)\n",
    "    tokens = stop_words_remover.remove_stopwords(tokens)\n",
    "    messages_sent.append(tokens)\n",
    "\n",
    "messages = []\n",
    "for tokens in messages_sent:\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [roof_removal.remove(token) for token in tokens]\n",
    "    tokens = [token_grouping.group_tokens(token) for token in tokens]\n",
    "    messages.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BoW dictionary\n",
    "token_dict_msg = TokenDictionary(messages, dict_size=512)\n",
    "token_dict_context = TokenDictionary(list(book_tokens.values()) + list(topics_tokens.values()), dict_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BoW of ngrams\n",
    "ngram_arr = F.build_ngram_model([msg.split() for msg in df.Message], 2)\n",
    "ngram_dict = TokenDictionary(ngram_arr, dict_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tf-idf weighted BoW representations\n",
    "bow = np.stack([token_dict_msg.bag_of_words(message) for message in messages])\n",
    "bow_tfidf = np.stack([token_dict_msg.bag_of_words(message, tf_idf=True) for message in messages])\n",
    "\n",
    "context_bow = np.stack([token_dict_context.bag_of_words(message) for message in messages])\n",
    "context_bow_tfidf = np.stack([token_dict_context.bag_of_words(message, tf_idf=True) for message in messages])\n",
    "\n",
    "# Get ngram model\n",
    "ngram_model = np.stack([ngram_dict.bag_of_words(ngram) for ngram in ngram_arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OŠ Ketteja in Murna' 'OŠ Franca Rozmana Staneta' 'OŠ Nove Fužine'\n",
      " 'OŠ Alojzija Šuštarja' 'OŠ Vižmarje - Brod' 'OŠ Vide Pregarc'\n",
      " 'OŠ Valentina Vodnika' 'OŠ Koseze']\n"
     ]
    }
   ],
   "source": [
    "# Create split\n",
    "kFolds = split_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "model1 = SklearnModel(LinearSVC(max_iter=4000))\n",
    "model2 = SklearnModel(RandomForestClassifier(n_estimators=500))\n",
    "model3 = CRFModel('test')\n",
    "\n",
    "models = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base feature functions\n",
    "fn_length = functools.partial(F.length, df=df)\n",
    "fn_wordcount = functools.partial(F.wordcount, df=df)\n",
    "fn_recent_activity = functools.partial(F.recent_activity, df=df)\n",
    "fn_sentiment = functools.partial(F.sentiment, messages=messages_sent, sentiment_analysis=sa, normalize=True)\n",
    "\n",
    "fn_msg_bow = functools.partial(F.message_bow, bow_values=bow_tfidf, name='bow_msg')\n",
    "fn_ngram_bow = functools.partial(F.message_bow, bow_values=ngram_model, name='bow_ngram')\n",
    "fn_context_bow = functools.partial(F.message_bow, bow_values=context_bow_tfidf, name='bow_ctx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature functions combinations\n",
    "feature_fn1 = F.merge_feature_functions([\n",
    "    fn_length,\n",
    "    fn_wordcount,\n",
    "    fn_sentiment,\n",
    "    fn_recent_activity\n",
    "])\n",
    "\n",
    "feature_fn2 = F.merge_feature_functions([\n",
    "    fn_msg_bow,\n",
    "    fn_ngram_bow\n",
    "])\n",
    "\n",
    "feature_fn3 = F.merge_feature_functions([\n",
    "    fn_length,\n",
    "    fn_wordcount,\n",
    "    fn_msg_bow,\n",
    "    fn_ngram_bow,\n",
    "    fn_sentiment,\n",
    "    fn_recent_activity,\n",
    "    fn_context_bow\n",
    "])\n",
    "\n",
    "\n",
    "features_fns = [feature_fn1, feature_fn2, feature_fn3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels_fn = functools.partial(F.get_label, df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators_train = []\n",
    "evaluators_test = []\n",
    "\n",
    "for i, features_fn in enumerate(features_fns):\n",
    "    evals_train = []\n",
    "    evals_test = []\n",
    "    \n",
    "    # Get datasets\n",
    "    folds_data = []\n",
    "    print('Preparing data...')\n",
    "    for train_dfs, test_dfs in tqdm(kFolds):\n",
    "        X_train, y_train = F.dataframes_to_dataset(train_dfs, features_fn, labels_fn)\n",
    "        X_test, y_test = F.dataframes_to_dataset(test_dfs, features_fn, labels_fn)\n",
    "\n",
    "        data = ((X_train, y_train), (X_test,y_test))\n",
    "        folds_data.append(data)\n",
    "        \n",
    "    for j, model in enumerate(models):\n",
    "        print(f\"Model {j}, feature set {i}\")\n",
    "        res = cross_validate(folds_data, model)\n",
    "        train_preds, train_labels, test_preds, test_labels = res\n",
    "\n",
    "        test_evaluator = Evaluator(test_preds, test_labels, tags)\n",
    "        evals_test.append(test_evaluator)\n",
    "        \n",
    "        train_evaluator = Evaluator(train_preds, train_labels, tags)\n",
    "        evals_train.append(train_evaluator)\n",
    "        \n",
    "        test_evaluator.get_classification_report(plot=True)\n",
    "        train_evaluator.get_classification_report(plot=True)\n",
    "\n",
    "    evaluators_train.append(evals_train)\n",
    "    evaluators_test.append(evals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp36",
   "language": "python",
   "name": "nlp36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
